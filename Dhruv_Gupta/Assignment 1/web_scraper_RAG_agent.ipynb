{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49dffdc7",
   "metadata": {},
   "source": [
    "### Second part of the assignment\n",
    "\n",
    "## Web Scraper + RAG Agent Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6526ebe",
   "metadata": {},
   "source": [
    "This agent:\n",
    "- uses a websearch agent (duckduckgo search), to find  5 relevant site URLs for a given query\n",
    "- scrapes the main textual content from these URLs\n",
    "- Applys a RAG pipeline to this scraped data in context of the query given\n",
    "  - chunks, embeds and retrieves the most relevant context for the LLM (groq)\n",
    "  - gives a final answer based on this RAG pipeline\n",
    "- (using `beautifulsoup4` for scraping data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "efa0261e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from duckduckgo_search import DDGS\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.docstore.document import Document\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "\n",
    "load_dotenv()\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "llm = ChatGroq(\n",
    "    #model_name=\"llama-3.3-70b-versatile\",\n",
    "    model_name=\"gemma2-9b-it\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=Warning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2ac9ed",
   "metadata": {},
   "source": [
    "### WebSearch Agent to find top 5 URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c795a08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddg_url_search(query: str, max_results: int = 5):\n",
    "    try:\n",
    "        with DDGS() as ddgs:\n",
    "            results = ddgs.text(query, region='wt-wt', safesearch=\"off\", max_results=max_results)\n",
    "            urls = []\n",
    "            for r in results:\n",
    "                url = r.get(\"href\", \"\")\n",
    "                if url and url.startswith(\"http\"):\n",
    "                    urls.append(url)\n",
    "            return urls if urls else []\n",
    "    except Exception as e:\n",
    "        print(f\"Error during DDG search: {e}\")\n",
    "        return [] \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd5cc50",
   "metadata": {},
   "source": [
    "### WebScraper to extract main textual content from top 5 URLs\n",
    "\n",
    "(Using requests + BeautifulSoup to get readable text from each URL.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f8c1e27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_from_url(url: str) -> str:\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=10, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "        \n",
    "        # remove script and style elements\n",
    "        for tag in soup(['script', 'style', 'head', 'meta', 'noscript']):\n",
    "            tag.decompose()\n",
    "        # get all the text\n",
    "        text = soup.get_text(separator='\\n')\n",
    "        # cleaning the text\n",
    "        text = re.sub(r'\\s+','',text)\n",
    "        \n",
    "        # truncate long outputs (not sure if this is removing context, but setting it to max 3000 for now)\n",
    "        if len(text) > 3000:\n",
    "            text = text[:3000]\n",
    "            print(\"Scraping Complete! Text is truncated to 3000 characters.\")\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {str(e)}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3965f626",
   "metadata": {},
   "source": [
    "### Creating the RAG document store\n",
    "(using `cohere` for vector embeddings, and `FAISS` as the vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "428c6c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")\n",
    "\n",
    "def create_rag_store_cohere(scraped_texts, urls):\n",
    "    docs = []\n",
    "    for txt, url in zip(scraped_texts, urls):\n",
    "        if not txt.startswith(\"ERROR\") and len(txt.strip()) > 0:\n",
    "            docs.append(Document(page_content=txt, metadata={\"source\": url}))\n",
    "    if not docs:\n",
    "        raise ValueError(\"No valid documents to index! Please check the scraping step and input URLs.\")\n",
    "    embeddings = CohereEmbeddings(model=\"embed-english-v3.0\")\n",
    "    vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7737dd10",
   "metadata": {},
   "source": [
    "### RAG Retrieval and QA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "30917a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_answer(query:str, vectorstore):\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\":5})\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    return qa_chain({\"query\":query})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa435f3",
   "metadata": {},
   "source": [
    "### Full agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d8bcdd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(query):\n",
    "    urls = ddg_url_search(query)\n",
    "    if not urls:\n",
    "        return \"No relevant URLs found.\"\n",
    "    \n",
    "    scraped_texts = [scrape_from_url(url) for url in urls]\n",
    "    vectorstore = create_rag_store(scraped_texts, urls) \n",
    "    if not vectorstore:\n",
    "        return \"No valid documents found to create a vector store.\"\n",
    "    \n",
    "    answer = rag_answer(query, vectorstore)\n",
    "    if not answer:\n",
    "        return \"No answer found.\"\n",
    "    \n",
    "    response = f\"Answer: {answer['result']}\\n\\nSources:\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23506dd",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e1a2f814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============Step 1: Searching for 5 relevant URLs...========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7b/7_944yjs1nbdx3s0b1jsvzyw0000gn/T/ipykernel_46238/2826494174.py:3: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  with DDGS() as ddgs:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found URLs: ['https://indianexpress.com/section/technology/artificial-intelligence/', 'https://news.google.com/topics/CAAqJAgKIh5DQkFTRUFvSEwyMHZNRzFyZWhJRlpXNHRSMElvQUFQAQ', 'https://aimagazine.com/articles/this-weeks-top-five-stories-in-ai', 'https://datahubanalytics.com/top-10-ai-developments-in-the-past-week-week-1-march-2025/', 'https://www.sciencedaily.com/news/computers_math/artificial_intelligence/']\n",
      "============Step 2: Scraping sites...==========\n",
      "\n",
      "Scraping Complete! Text is truncated to 3000 characters.\n",
      "Scraping Complete! Text is truncated to 3000 characters.\n",
      "Scraping Complete! Text is truncated to 3000 characters.\n",
      "Scraping Complete! Text is truncated to 3000 characters.\n",
      "Scraping Complete! Text is truncated to 3000 characters.\n",
      "============Step 3: Building RAG vector store...========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/newvenv/lib/python3.13/site-packages/cohere/core/pydantic_utilities.py:240: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  return cast(Mapping[str, PydanticField], model.model_fields)  # type: ignore[attr-defined]\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/newvenv/lib/python3.13/site-packages/cohere/utils.py:233: PydanticDeprecatedSince20: The `parse_obj` method is deprecated; use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  embeddings_by_type_merged = EmbedByTypeResponseEmbeddings.parse_obj(merged_dicts)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========Step 4: Asking final question to RAG + LLM agent...==========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/newvenv/lib/python3.13/site-packages/cohere/core/pydantic_utilities.py:240: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  return cast(Mapping[str, PydanticField], model.model_fields)  # type: ignore[attr-defined]\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/newvenv/lib/python3.13/site-packages/cohere/utils.py:233: PydanticDeprecatedSince20: The `parse_obj` method is deprecated; use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  embeddings_by_type_merged = EmbedByTypeResponseEmbeddings.parse_obj(merged_dicts)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Answer:\n",
      " Here are some AI research highlights from the past 3 months, based on the provided text: \n",
      "\n",
      "* **Empathy in AI:** Experts like Geoffrey Hinton (considered the \"Godfather of AI\") and Yann LeCun (Meta's AI chief) emphasize the importance of empathy in AI development to prevent negative consequences.\n",
      "* **AI-Powered Crime Prediction:** The UK government is developing an AI-powered system to predict criminal activity like theft, knife attacks, and violent crimes.\n",
      "* **Fake Job Applicant Profiles:** Gartner predicts that by 2028, 25% of job applicant profiles will be AI-generated.\n",
      "* **AI for Job Seekers:** There's growing concern about AI tools being used to create fake job applicant profiles and potentially deceive employers.\n",
      "* **Small AI Models for Mobile Devices:** Google released Gemma3270M, a compact AI model designed to run efficiently on mobile phones and edge devices. \n",
      "* **OpenAI and India:** OpenAI CEO Sam Altman talked about the immense potential of AI in India, highlighting the opportunities for young people. He also discussed the importance of responsible AI development and the need to address ethical concerns.\n",
      "\n",
      "**Note:** This list is limited by the information provided in the text. There are many other AI research developments happening globally. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Source Documents:\n",
      "- Source: https://www.sciencedaily.com/news/computers_math/artificial_intelligence/\n",
      "- Source: https://aimagazine.com/articles/this-weeks-top-five-stories-in-ai\n",
      "- Source: https://datahubanalytics.com/top-10-ai-developments-in-the-past-week-week-1-march-2025/\n",
      "- Source: https://indianexpress.com/section/technology/artificial-intelligence/\n",
      "- Source: https://news.google.com/topics/CAAqJAgKIh5DQkFTRUFvSEwyMHZNRzFyZWhJRlpXNHRSMElvQUFQAQ\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the latest AI research from within 3 months ago?\"\n",
    "\n",
    "print(\"============Step 1: Searching for 5 relevant URLs...========\\n\")\n",
    "urls = ddg_url_search(question)\n",
    "print(\"Found URLs:\", urls)\n",
    "\n",
    "print(\"============Step 2: Scraping sites...==========\\n\")\n",
    "scraped = [scrape_from_url(u) for u in urls]\n",
    "\n",
    "print(\"============Step 3: Building RAG vector store...========\\n\")\n",
    "vector_db = create_rag_store(scraped, urls)\n",
    "\n",
    "print(\"========Step 4: Asking final question to RAG + LLM agent...==========\\n\")\n",
    "result = rag_answer(question, vector_db)\n",
    "\n",
    "print(\"\\nGenerated Answer:\\n\", result['result'])\n",
    "print(\"\\nSource Documents:\")\n",
    "for doc in result['source_documents']:\n",
    "    print(f\"- Source: {doc.metadata['source']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66de9a16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
